{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11527333,"sourceType":"datasetVersion","datasetId":7229767}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T10:32:57.931774Z","iopub.execute_input":"2025-04-23T10:32:57.932116Z","iopub.status.idle":"2025-04-23T10:32:59.559963Z","shell.execute_reply.started":"2025-04-23T10:32:57.932092Z","shell.execute_reply":"2025-04-23T10:32:59.558595Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/status-output-1/status_output_small.csv\n/kaggle/input/status-output-1/status_output.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/status-output-1/status_output.csv\")\n\n# Columns\nstep_cols = [f'Step_{i}' for i in range(1, 21)]\ninput_cols = ['PIs', 'POs', 'AND_Gates_Before', 'Levels_Before'] + step_cols\ntarget_col = 'Power'\n\n# Encode the steps to integer tokens\nall_steps = sorted(set(step for row in df[step_cols].values for step in row))\nstep_encoder = {step: idx + 100 for idx, step in enumerate(all_steps)}  # reserve 0-99 for scalar values\n\n# Encode steps\nencoded_steps = df[step_cols].applymap(lambda s: step_encoder[s])\n\n# Normalize and scale scalar inputs to avoid huge embedding indices\nscalar_inputs = df[['PIs', 'POs', 'AND_Gates_Before', 'Levels_Before']].copy()\nscalar_inputs = scalar_inputs.apply(lambda col: col - col.min() + 1)\n\n# Concatenate scalar inputs and steps\ndf_encoded = pd.concat([scalar_inputs, encoded_steps], axis=1)\n\n# Dataset class\nclass PowerDataset(Dataset):\n    def __init__(self, features, targets):\n        self.X = features\n        self.y = targets\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x_seq = torch.tensor(self.X[idx], dtype=torch.long)\n        y_val = torch.tensor(self.y[idx], dtype=torch.float32)\n        return x_seq, y_val\n\n# Normalize QoR target to 0–1 range\nqor_min = df[target_col].min()\nqor_max = df[target_col].max()\ndf[target_col + \"_scaled\"] = (df[target_col] - qor_min) / (qor_max - qor_min)\n\n# Prepare data\nX_raw = df_encoded.values.tolist()\ny_raw = df[target_col + \"_scaled\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)\n\ntrain_dataset = PowerDataset(X_train, y_train)\ntest_dataset = PowerDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# === Load and preprocess NEW dataset the same way ===\nnew_df = pd.read_csv(\"/kaggle/input/status-output-1/status_output_small.csv\")  # Update to your actual file path\n\n# Step encoding using the existing encoder, with handling for unknowns\nstep_encoder = {step: idx + 100 for idx, step in enumerate(all_steps)}  # Ensure it's outside the scalar range\nstep_encoder[\"unknown\"] = len(step_encoder) + 100  # Reserve an index for unknown steps\nnew_encoded_steps = new_df[step_cols].applymap(lambda s: step_encoder.get(s, step_encoder[\"unknown\"]))\n\n# Normalize scalar inputs\nnew_scalar_inputs = new_df[['PIs', 'POs', 'AND_Gates_Before', 'Levels_Before']].copy()\nnew_scalar_inputs = new_scalar_inputs.apply(lambda col: col - col.min() + 1)\n\n# Concatenate\nnew_df_encoded = pd.concat([new_scalar_inputs, new_encoded_steps], axis=1)\n\n# Normalize power using original scaling\nnew_df[target_col + \"_scaled\"] = (new_df[target_col] - qor_min) / (qor_max - qor_min)\n\n# Prepare new data\nX_new_raw = new_df_encoded.values.tolist()\ny_new_raw = new_df[target_col + \"_scaled\"].values\n\n# Determine the maximum step index in the new dataset\nmax_step_value_new = new_encoded_steps.values.max()\n\n# Train-test split\nX_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_new_raw, y_new_raw, test_size=0.2, random_state=42)\n\n# Dataset and loaders\ntrain_new_dataset = PowerDataset(X_new_train, y_new_train)\ntest_new_dataset = PowerDataset(X_new_test, y_new_test)\n\ntrain_new_loader = DataLoader(train_new_dataset, batch_size=32, shuffle=True)\ntest_new_loader = DataLoader(test_new_dataset, batch_size=32)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:01:37.679918Z","iopub.execute_input":"2025-04-23T11:01:37.680245Z","iopub.status.idle":"2025-04-23T11:01:37.788556Z","shell.execute_reply.started":"2025-04-23T11:01:37.680218Z","shell.execute_reply":"2025-04-23T11:01:37.787596Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1805548282.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  encoded_steps = df[step_cols].applymap(lambda s: step_encoder[s])\n/tmp/ipykernel_31/1805548282.py:67: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  new_encoded_steps = new_df[step_cols].applymap(lambda s: step_encoder.get(s, step_encoder[\"unknown\"]))\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#Changes made:\n#Switched to a bidirectional GRU to capture context from both directions.\n#Stacked 2 GRU layers for more modeling power.\n#Added dropout for regularization.\n#Used a Linear → ReLU → Linear head for better learning capacity.\n\nclass PowerCustomRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, num_layers=2, dropout=0.3):\n        super(PowerCustomRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.GRU(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            dropout=dropout,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n        _, hidden = self.rnn(embedded)  # hidden shape: [num_layers*2, batch, hidden_dim]\n        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)  # concatenate last layers of both directions\n        return self.fc(hidden_cat)\n\n# Ensure vocab_size is large enough to cover all indices\nvocab_size = max(max(df_encoded.values.flatten()) + 1, max(new_df_encoded.values.flatten()) + 1, max_step_value_new + 1, 1000)\nmodel = PowerCustomRNN(vocab_size)\n\n# Training\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# === Training with Early Stopping ===\nbest_loss = float('inf')\npatience = 10\ncounter = 0\n\nfor epoch in range(300):\n    model.train()\n    total_loss = 0\n    for batch_x, batch_y in train_loader:\n        optimizer.zero_grad()\n        output = model(batch_x).squeeze()\n        loss = criterion(output, batch_y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}/300, Loss: {total_loss:.4f}\")\n\n    # Early stopping\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_x, batch_y in test_loader:\n            output = model(batch_x).squeeze()\n            val_loss += criterion(output, batch_y).item()\n    val_loss /= len(test_loader)\n\n    if val_loss < best_loss:\n        best_loss = val_loss\n        counter = 0\n        best_model_state = model.state_dict()\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            model.load_state_dict(best_model_state)\n            break\n\n# === Evaluation ===\nmodel.eval()\nactuals = []\npredictions = []\n\nwith torch.no_grad():\n    for batch_x, batch_y in test_loader:\n        output = model(batch_x).squeeze()\n        actuals.extend(batch_y.cpu().numpy())\n        predictions.extend(output.cpu().numpy())\n\nactuals = np.array(actuals)\npredictions = np.array(predictions)\nnonzero_mask = actuals != 0\nmape = np.mean(np.abs((actuals[nonzero_mask] - predictions[nonzero_mask]) / actuals[nonzero_mask])) * 100\nmapa = 100 - mape\nprint(f\"Mean Absolute Percentage Accuracy (MAPA): {mapa:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:07:09.409123Z","iopub.execute_input":"2025-04-23T11:07:09.409800Z","iopub.status.idle":"2025-04-23T11:12:38.563694Z","shell.execute_reply.started":"2025-04-23T11:07:09.409770Z","shell.execute_reply":"2025-04-23T11:12:38.562749Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/300, Loss: 0.8040\nEpoch 2/300, Loss: 0.1811\nEpoch 3/300, Loss: 0.1050\nEpoch 4/300, Loss: 0.0805\nEpoch 5/300, Loss: 0.0743\nEpoch 6/300, Loss: 0.0564\nEpoch 7/300, Loss: 0.0542\nEpoch 8/300, Loss: 0.0712\nEpoch 9/300, Loss: 0.0429\nEpoch 10/300, Loss: 0.0383\nEpoch 11/300, Loss: 0.0450\nEpoch 12/300, Loss: 0.0370\nEpoch 13/300, Loss: 0.0344\nEpoch 14/300, Loss: 0.0290\nEpoch 15/300, Loss: 0.0272\nEpoch 16/300, Loss: 0.0306\nEpoch 17/300, Loss: 0.0298\nEpoch 18/300, Loss: 0.0275\nEpoch 19/300, Loss: 0.0231\nEpoch 20/300, Loss: 0.0225\nEpoch 21/300, Loss: 0.0168\nEpoch 22/300, Loss: 0.0188\nEpoch 23/300, Loss: 0.0196\nEpoch 24/300, Loss: 0.0176\nEpoch 25/300, Loss: 0.0175\nEpoch 26/300, Loss: 0.0145\nEpoch 27/300, Loss: 0.0141\nEpoch 28/300, Loss: 0.0115\nEpoch 29/300, Loss: 0.0136\nEpoch 30/300, Loss: 0.0130\nEpoch 31/300, Loss: 0.0133\nEpoch 32/300, Loss: 0.0132\nEpoch 33/300, Loss: 0.0137\nEpoch 34/300, Loss: 0.0118\nEpoch 35/300, Loss: 0.0116\nEpoch 36/300, Loss: 0.0129\nEpoch 37/300, Loss: 0.0105\nEpoch 38/300, Loss: 0.0108\nEpoch 39/300, Loss: 0.0166\nEpoch 40/300, Loss: 0.0218\nEpoch 41/300, Loss: 0.0132\nEpoch 42/300, Loss: 0.0105\nEpoch 43/300, Loss: 0.0081\nEpoch 44/300, Loss: 0.0111\nEpoch 45/300, Loss: 0.0120\nEpoch 46/300, Loss: 0.0092\nEpoch 47/300, Loss: 0.0084\nEpoch 48/300, Loss: 0.0081\nEpoch 49/300, Loss: 0.0080\nEpoch 50/300, Loss: 0.0099\nEpoch 51/300, Loss: 0.0080\nEpoch 52/300, Loss: 0.0055\nEpoch 53/300, Loss: 0.0046\nEpoch 54/300, Loss: 0.0058\nEarly stopping at epoch 54\nMean Absolute Percentage Accuracy (MAPA): 90.44%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nmodel.eval()\n\n# Fine-tuning the model with the new dataset\nfine_tune_epochs = 100\nfor epoch in range(fine_tune_epochs):\n    model.train()\n    total_loss = 0\n    for batch_x, batch_y in train_new_loader:  # Use train_new_loader\n        batch_x = batch_x.long()  # Ensure the batch is in the correct type (long tensor for indices)\n        optimizer.zero_grad()\n        output = model(batch_x).squeeze()  # Forward pass\n        loss = criterion(output, batch_y)  # Compute loss\n        loss.backward()  # Backpropagate\n        optimizer.step()  # Update weights\n        total_loss += loss.item()  # Accumulate the loss\n    print(f\"Fine-tune Epoch {epoch+1}/{fine_tune_epochs}, Loss: {total_loss:.4f}\")\n\n# Evaluation on new test set\nmodel.eval()\nactuals = []\npredictions = []\n\nwith torch.no_grad():\n    for batch_x, batch_y in test_new_loader:  # Evaluate on test_new_loader\n        batch_x = batch_x.long()  # Ensure the batch is in the correct type (long tensor for indices)\n        output = model(batch_x).squeeze()\n        actuals.extend(batch_y.cpu().numpy())\n        predictions.extend(output.cpu().numpy())\n\nactuals = np.array(actuals)\npredictions = np.array(predictions)\n\n# Avoid division by zero\nnonzero_mask = actuals != 0\nmape = np.mean(np.abs((actuals[nonzero_mask] - predictions[nonzero_mask]) / actuals[nonzero_mask])) * 100\nmapa = 100 - mape\n\nprint(f\"Mean Absolute Percentage Accuracy (MAPA): {mapa:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:13:11.445881Z","iopub.execute_input":"2025-04-23T11:13:11.446741Z","iopub.status.idle":"2025-04-23T11:14:11.788040Z","shell.execute_reply.started":"2025-04-23T11:13:11.446704Z","shell.execute_reply":"2025-04-23T11:14:11.787223Z"}},"outputs":[{"name":"stdout","text":"Fine-tune Epoch 1/100, Loss: 31.8651\nFine-tune Epoch 2/100, Loss: 4.8504\nFine-tune Epoch 3/100, Loss: 0.7470\nFine-tune Epoch 4/100, Loss: 0.2224\nFine-tune Epoch 5/100, Loss: 0.1334\nFine-tune Epoch 6/100, Loss: 0.0990\nFine-tune Epoch 7/100, Loss: 0.0974\nFine-tune Epoch 8/100, Loss: 0.0902\nFine-tune Epoch 9/100, Loss: 0.0778\nFine-tune Epoch 10/100, Loss: 0.0728\nFine-tune Epoch 11/100, Loss: 0.0730\nFine-tune Epoch 12/100, Loss: 0.0719\nFine-tune Epoch 13/100, Loss: 0.0600\nFine-tune Epoch 14/100, Loss: 0.0441\nFine-tune Epoch 15/100, Loss: 0.0380\nFine-tune Epoch 16/100, Loss: 0.0427\nFine-tune Epoch 17/100, Loss: 0.0372\nFine-tune Epoch 18/100, Loss: 0.0341\nFine-tune Epoch 19/100, Loss: 0.0346\nFine-tune Epoch 20/100, Loss: 0.0331\nFine-tune Epoch 21/100, Loss: 0.0290\nFine-tune Epoch 22/100, Loss: 0.0368\nFine-tune Epoch 23/100, Loss: 0.0320\nFine-tune Epoch 24/100, Loss: 0.0267\nFine-tune Epoch 25/100, Loss: 0.0232\nFine-tune Epoch 26/100, Loss: 0.0228\nFine-tune Epoch 27/100, Loss: 0.0231\nFine-tune Epoch 28/100, Loss: 0.0265\nFine-tune Epoch 29/100, Loss: 0.0197\nFine-tune Epoch 30/100, Loss: 0.0211\nFine-tune Epoch 31/100, Loss: 0.0192\nFine-tune Epoch 32/100, Loss: 0.0198\nFine-tune Epoch 33/100, Loss: 0.0209\nFine-tune Epoch 34/100, Loss: 0.0172\nFine-tune Epoch 35/100, Loss: 0.0153\nFine-tune Epoch 36/100, Loss: 0.0170\nFine-tune Epoch 37/100, Loss: 0.0167\nFine-tune Epoch 38/100, Loss: 0.0157\nFine-tune Epoch 39/100, Loss: 0.0170\nFine-tune Epoch 40/100, Loss: 0.0123\nFine-tune Epoch 41/100, Loss: 0.0132\nFine-tune Epoch 42/100, Loss: 0.0120\nFine-tune Epoch 43/100, Loss: 0.0100\nFine-tune Epoch 44/100, Loss: 0.0140\nFine-tune Epoch 45/100, Loss: 0.0136\nFine-tune Epoch 46/100, Loss: 0.0125\nFine-tune Epoch 47/100, Loss: 0.0113\nFine-tune Epoch 48/100, Loss: 0.0111\nFine-tune Epoch 49/100, Loss: 0.0114\nFine-tune Epoch 50/100, Loss: 0.0149\nFine-tune Epoch 51/100, Loss: 0.0120\nFine-tune Epoch 52/100, Loss: 0.0122\nFine-tune Epoch 53/100, Loss: 0.0087\nFine-tune Epoch 54/100, Loss: 0.0105\nFine-tune Epoch 55/100, Loss: 0.0095\nFine-tune Epoch 56/100, Loss: 0.0101\nFine-tune Epoch 57/100, Loss: 0.0086\nFine-tune Epoch 58/100, Loss: 0.0067\nFine-tune Epoch 59/100, Loss: 0.0076\nFine-tune Epoch 60/100, Loss: 0.0121\nFine-tune Epoch 61/100, Loss: 0.0114\nFine-tune Epoch 62/100, Loss: 0.0093\nFine-tune Epoch 63/100, Loss: 0.0074\nFine-tune Epoch 64/100, Loss: 0.0079\nFine-tune Epoch 65/100, Loss: 0.0098\nFine-tune Epoch 66/100, Loss: 0.0086\nFine-tune Epoch 67/100, Loss: 0.0086\nFine-tune Epoch 68/100, Loss: 0.0076\nFine-tune Epoch 69/100, Loss: 0.0066\nFine-tune Epoch 70/100, Loss: 0.0076\nFine-tune Epoch 71/100, Loss: 0.0058\nFine-tune Epoch 72/100, Loss: 0.0066\nFine-tune Epoch 73/100, Loss: 0.0059\nFine-tune Epoch 74/100, Loss: 0.0048\nFine-tune Epoch 75/100, Loss: 0.0057\nFine-tune Epoch 76/100, Loss: 0.0073\nFine-tune Epoch 77/100, Loss: 0.0069\nFine-tune Epoch 78/100, Loss: 0.0070\nFine-tune Epoch 79/100, Loss: 0.0060\nFine-tune Epoch 80/100, Loss: 0.0062\nFine-tune Epoch 81/100, Loss: 0.0068\nFine-tune Epoch 82/100, Loss: 0.0063\nFine-tune Epoch 83/100, Loss: 0.0051\nFine-tune Epoch 84/100, Loss: 0.0046\nFine-tune Epoch 85/100, Loss: 0.0045\nFine-tune Epoch 86/100, Loss: 0.0050\nFine-tune Epoch 87/100, Loss: 0.0048\nFine-tune Epoch 88/100, Loss: 0.0059\nFine-tune Epoch 89/100, Loss: 0.0049\nFine-tune Epoch 90/100, Loss: 0.0040\nFine-tune Epoch 91/100, Loss: 0.0056\nFine-tune Epoch 92/100, Loss: 0.0050\nFine-tune Epoch 93/100, Loss: 0.0055\nFine-tune Epoch 94/100, Loss: 0.0066\nFine-tune Epoch 95/100, Loss: 0.0041\nFine-tune Epoch 96/100, Loss: 0.0053\nFine-tune Epoch 97/100, Loss: 0.0051\nFine-tune Epoch 98/100, Loss: 0.0047\nFine-tune Epoch 99/100, Loss: 0.0041\nFine-tune Epoch 100/100, Loss: 0.0056\nMean Absolute Percentage Accuracy (MAPA): 96.35%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}