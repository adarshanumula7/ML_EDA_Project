# QoR Power Estimation using Machine Learning

This project demonstrates a machine learning-based approach for estimating **Quality of Results (QoR)** â€” specifically **power consumption** â€” of logic synthesis flows. We use synthesis recipes (transformation sequences) as sequential input and predict power after synthesis using deep learning models.

## ğŸ“Œ Problem Statement

In logic synthesis, early prediction of QoR metrics (power, area, delay) can guide optimization and reduce costly iterations. This project proposes an ML pipeline to predict **power** using synthesis recipe sequences and design features.

## ğŸ§° Technologies Used

- Python, Pandas, NumPy
- Kaggle Notebook (for model development)
- ABC logic synthesis tool (for extracting QoR)



## ğŸ“ Dataset

- `status_output.csv` (3000 samples):

  - Scalar features: Primary Inputs (PIs), Outputs (POs), AND Gates, Levels
  - Sequential features: Step\_1 to Step\_20 (synthesis transformations)
  - Target: Power (normalized)

- `status_output_small.csv`:

  - Used for **fine-tuning** models on new, unseen designs

## ğŸ§ª Models Implemented

### ğŸ”¹ Simple RNN (GRU-based)

- Embedding â†’ GRU(128) â†’ Linear
- MAPA (Accuracy): **88.69%**
- Post Fine-Tuning MAPA: **94.82%**

### ğŸ”¹ Custom RNN

- Bidirectional GRU (stacked) â†’ Dropout â†’ Linear â†’ ReLU â†’ Linear
- MAPA (Accuracy): **90.21%**
- Post Fine-Tuning MAPA: **96.35%**

### ğŸ”¹ Other Models (Not detailed in repo but explored in report)

- LSTM
- Transformer

## ğŸ§¹ Preprocessing Steps

1. Step Encoding (unique transformation names â†’ tokens)
2. Scalar Normalization (PIs, POs, etc.)
3. Power normalization (0â€“1 range)
4. Merging all inputs for embedding
5. Train/test split (80/20)
6. Custom PyTorch Dataset + DataLoader

## ğŸ” Fine-Tuning

We use `status_output_small.csv` to fine-tune pretrained models on a new design:

- Allows **domain adaptation** and boosts accuracy
- Done by retraining on small design-specific dataset
- Models significantly improve generalization

## ğŸ“ˆ Results Summary

| Model      | MAPA (Before) | MAPA (After Fine-Tuning) |
| ---------- | ------------- | ------------------------ |
| Simple RNN | 88.69%        | 94.82%                   |
| Custom RNN | 90.21%        | 96.35%                   |

## ğŸ“Œ Future Work

- Add more benchmarks
- Include area and delay as additional targets
- Explore hybrid or pretrained models
- Deploy as a web-based synthesis assistant tool

## ğŸ‘¨â€ğŸ’» Contributors

- Anumula Adarsh (210101018)
- Arani Rajesh Kumar (210101020)
- Majji Aditya (210101064)

---

