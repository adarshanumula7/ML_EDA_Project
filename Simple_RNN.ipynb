{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11526336,"sourceType":"datasetVersion","datasetId":7228972},{"sourceId":11526971,"sourceType":"datasetVersion","datasetId":7229474}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/status-output/status_output.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:21:26.565792Z","iopub.execute_input":"2025-04-23T11:21:26.566097Z","iopub.status.idle":"2025-04-23T11:21:26.602363Z","shell.execute_reply.started":"2025-04-23T11:21:26.566073Z","shell.execute_reply":"2025-04-23T11:21:26.601431Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:21:28.058139Z","iopub.execute_input":"2025-04-23T11:21:28.059091Z","iopub.status.idle":"2025-04-23T11:21:28.098576Z","shell.execute_reply.started":"2025-04-23T11:21:28.059060Z","shell.execute_reply":"2025-04-23T11:21:28.097760Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   PIs  POs  AND_Gates_Before  Levels_Before    Step_1     Step_2     Step_3  \\\n0   24   25              5416            225  resub -z        drw      resyn   \n1  207  107              2198             30     share  compress2      share   \n2   26   11              1592             31  compress      rwsat  compress2   \n3   24   25              5416            225       drw      share      share   \n4  207  107              2198             30       dc2     choice     strash   \n\n     Step_4   Step_5    Step_6  ...    Step_17      Step_18    Step_19  \\\n0  resub -z   resyn3   drwsat2  ...  compress2      balance   resub -z   \n1  compress  resyn2a   balance  ...     choice       strash    balance   \n2  refactor      drf    choice  ...    rewrite  refactor -z  compress2   \n3   choice2   strash  resub -z  ...     strash        resyn  compress2   \n4  resub -z  rewrite       drf  ...      resyn        rwsat     resyn2   \n\n    Step_20    ND  Edge    Area Delay Levels_After    Power  \n0     share  3295  8733  3295.0  81.0           81  3219.53  \n1   drwsat2   800  1805   800.0  22.0           22   934.57  \n2  refactor   551  1505   551.0  10.0           10   700.42  \n3       dc2  3309  8676  3309.0  81.0           81  3208.00  \n4       drf   803  1782   803.0  24.0           24   920.30  \n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PIs</th>\n      <th>POs</th>\n      <th>AND_Gates_Before</th>\n      <th>Levels_Before</th>\n      <th>Step_1</th>\n      <th>Step_2</th>\n      <th>Step_3</th>\n      <th>Step_4</th>\n      <th>Step_5</th>\n      <th>Step_6</th>\n      <th>...</th>\n      <th>Step_17</th>\n      <th>Step_18</th>\n      <th>Step_19</th>\n      <th>Step_20</th>\n      <th>ND</th>\n      <th>Edge</th>\n      <th>Area</th>\n      <th>Delay</th>\n      <th>Levels_After</th>\n      <th>Power</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24</td>\n      <td>25</td>\n      <td>5416</td>\n      <td>225</td>\n      <td>resub -z</td>\n      <td>drw</td>\n      <td>resyn</td>\n      <td>resub -z</td>\n      <td>resyn3</td>\n      <td>drwsat2</td>\n      <td>...</td>\n      <td>compress2</td>\n      <td>balance</td>\n      <td>resub -z</td>\n      <td>share</td>\n      <td>3295</td>\n      <td>8733</td>\n      <td>3295.0</td>\n      <td>81.0</td>\n      <td>81</td>\n      <td>3219.53</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>207</td>\n      <td>107</td>\n      <td>2198</td>\n      <td>30</td>\n      <td>share</td>\n      <td>compress2</td>\n      <td>share</td>\n      <td>compress</td>\n      <td>resyn2a</td>\n      <td>balance</td>\n      <td>...</td>\n      <td>choice</td>\n      <td>strash</td>\n      <td>balance</td>\n      <td>drwsat2</td>\n      <td>800</td>\n      <td>1805</td>\n      <td>800.0</td>\n      <td>22.0</td>\n      <td>22</td>\n      <td>934.57</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>11</td>\n      <td>1592</td>\n      <td>31</td>\n      <td>compress</td>\n      <td>rwsat</td>\n      <td>compress2</td>\n      <td>refactor</td>\n      <td>drf</td>\n      <td>choice</td>\n      <td>...</td>\n      <td>rewrite</td>\n      <td>refactor -z</td>\n      <td>compress2</td>\n      <td>refactor</td>\n      <td>551</td>\n      <td>1505</td>\n      <td>551.0</td>\n      <td>10.0</td>\n      <td>10</td>\n      <td>700.42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24</td>\n      <td>25</td>\n      <td>5416</td>\n      <td>225</td>\n      <td>drw</td>\n      <td>share</td>\n      <td>share</td>\n      <td>choice2</td>\n      <td>strash</td>\n      <td>resub -z</td>\n      <td>...</td>\n      <td>strash</td>\n      <td>resyn</td>\n      <td>compress2</td>\n      <td>dc2</td>\n      <td>3309</td>\n      <td>8676</td>\n      <td>3309.0</td>\n      <td>81.0</td>\n      <td>81</td>\n      <td>3208.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>207</td>\n      <td>107</td>\n      <td>2198</td>\n      <td>30</td>\n      <td>dc2</td>\n      <td>choice</td>\n      <td>strash</td>\n      <td>resub -z</td>\n      <td>rewrite</td>\n      <td>drf</td>\n      <td>...</td>\n      <td>resyn</td>\n      <td>rwsat</td>\n      <td>resyn2</td>\n      <td>drf</td>\n      <td>803</td>\n      <td>1782</td>\n      <td>803.0</td>\n      <td>24.0</td>\n      <td>24</td>\n      <td>920.30</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Columns\nstep_cols = [f'Step_{i}' for i in range(1, 21)]\ninput_cols = ['PIs', 'POs', 'AND_Gates_Before', 'Levels_Before'] + step_cols\ntarget_col = 'Power'\n\n# Encode the steps to integer tokens\nall_steps = sorted(set(step for row in df[step_cols].values for step in row))\nstep_encoder = {step: idx + 100 for idx, step in enumerate(all_steps)}  # reserve 0-99 for scalar values\n\n# Encode steps\nencoded_steps = df[step_cols].applymap(lambda s: step_encoder[s])\n\n# Normalize and scale scalar inputs to avoid huge embedding indices\nscalar_inputs = df[['PIs', 'POs', 'AND_Gates_Before', 'Levels_Before']].copy()\nscalar_inputs = scalar_inputs.apply(lambda col: col - col.min() + 1)\n\n# Concatenate scalar inputs and steps\ndf_encoded = pd.concat([scalar_inputs, encoded_steps], axis=1)\n\n# Dataset class\nclass PowerDataset(Dataset):\n    def __init__(self, features, targets):\n        self.X = features\n        self.y = targets\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x_seq = torch.tensor(self.X[idx], dtype=torch.long)\n        y_val = torch.tensor(self.y[idx], dtype=torch.float32)\n        return x_seq, y_val\n\n# Normalize QoR target to 0–1 range\nqor_min = df[target_col].min()\nqor_max = df[target_col].max()\ndf[target_col + \"_scaled\"] = (df[target_col] - qor_min) / (qor_max - qor_min)\n\n# Prepare data\nX_raw = df_encoded.values.tolist()\ny_raw = df[target_col + \"_scaled\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)\n\ntrain_dataset = PowerDataset(X_train, y_train)\ntest_dataset = PowerDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# === Load and preprocess NEW dataset the same way ===\nnew_df = pd.read_csv(\"/kaggle/input/status-small/status_output_small.csv\")  # Update to your actual file path\n\n# Step encoding using the existing encoder, with handling for unknowns\nstep_encoder = {step: idx + 100 for idx, step in enumerate(all_steps)}  # Ensure it's outside the scalar range\nstep_encoder[\"unknown\"] = len(step_encoder) + 100  # Reserve an index for unknown steps\nnew_encoded_steps = new_df[step_cols].applymap(lambda s: step_encoder.get(s, step_encoder[\"unknown\"]))\n\n# Normalize scalar inputs\nnew_scalar_inputs = new_df[['PIs', 'POs', 'AND_Gates_Before', 'Levels_Before']].copy()\nnew_scalar_inputs = new_scalar_inputs.apply(lambda col: col - col.min() + 1)\n\n# Concatenate\nnew_df_encoded = pd.concat([new_scalar_inputs, new_encoded_steps], axis=1)\n\n# Normalize power using original scaling\nnew_df[target_col + \"_scaled\"] = (new_df[target_col] - qor_min) / (qor_max - qor_min)\n\n# Prepare new data\nX_new_raw = new_df_encoded.values.tolist()\ny_new_raw = new_df[target_col + \"_scaled\"].values\n\n# Determine the maximum step index in the new dataset\nmax_step_value_new = new_encoded_steps.values.max()\n\n# Train-test split\nX_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_new_raw, y_new_raw, test_size=0.2, random_state=42)\n\n# Dataset and loaders\ntrain_new_dataset = PowerDataset(X_new_train, y_new_train)\ntest_new_dataset = PowerDataset(X_new_test, y_new_test)\n\ntrain_new_loader = DataLoader(train_new_dataset, batch_size=32, shuffle=True)\ntest_new_loader = DataLoader(test_new_dataset, batch_size=32)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:21:36.270953Z","iopub.execute_input":"2025-04-23T11:21:36.271249Z","iopub.status.idle":"2025-04-23T11:21:36.361756Z","shell.execute_reply.started":"2025-04-23T11:21:36.271225Z","shell.execute_reply":"2025-04-23T11:21:36.360871Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/579936325.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  encoded_steps = df[step_cols].applymap(lambda s: step_encoder[s])\n/tmp/ipykernel_31/579936325.py:57: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  new_encoded_steps = new_df[step_cols].applymap(lambda s: step_encoder.get(s, step_encoder[\"unknown\"]))\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# RNN model\nclass PowerRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n        super(PowerRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, hidden = self.rnn(embedded)\n        return self.fc(hidden.squeeze(0))\n\n\n# Ensure vocab_size is large enough to cover all indices\nvocab_size = max(max(df_encoded.values.flatten()) + 1, max(new_df_encoded.values.flatten()) + 1, max_step_value_new + 1, 1000)\nmodel = PowerRNN(vocab_size)\n\n# Training\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# === Early Stopping Setup ===\nbest_val_loss = float('inf')\npatience = 20\ncounter = 0\n\n# === Training Loop with Early Stopping ===\nfor epoch in range(300):\n    model.train()\n    total_loss = 0\n    for batch_x, batch_y in train_loader:\n        optimizer.zero_grad()\n        output = model(batch_x).squeeze()\n        loss = criterion(output, batch_y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # === Validation Loss ===\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_x, batch_y in test_loader:\n            output = model(batch_x).squeeze()\n            loss = criterion(output, batch_y)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(test_loader)\n    print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n    # === Check for Early Stopping ===\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        counter = 0\n        best_model_state = model.state_dict()\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            model.load_state_dict(best_model_state)\n            break\n\n# === Final Evaluation (MAPA) ===\nmodel.eval()\nactuals = []\npredictions = []\n\nwith torch.no_grad():\n    for batch_x, batch_y in test_loader:\n        output = model(batch_x).squeeze()\n        actuals.extend(batch_y.cpu().numpy())\n        predictions.extend(output.cpu().numpy())\n\nactuals = np.array(actuals)\npredictions = np.array(predictions)\n\nnonzero_mask = actuals != 0\nmape = np.mean(np.abs((actuals[nonzero_mask] - predictions[nonzero_mask]) / actuals[nonzero_mask])) * 100\nmapa = 100 - mape\n\nprint(f\"Mean Absolute Percentage Accuracy (MAPA): {mapa:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:21:40.055446Z","iopub.execute_input":"2025-04-23T11:21:40.056142Z","iopub.status.idle":"2025-04-23T11:30:39.589836Z","shell.execute_reply.started":"2025-04-23T11:21:40.056104Z","shell.execute_reply":"2025-04-23T11:30:39.588998Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 6.6868, Val Loss: 0.0808\nEpoch 2, Train Loss: 6.1698, Val Loss: 0.0795\nEpoch 3, Train Loss: 6.1039, Val Loss: 0.0819\nEpoch 4, Train Loss: 5.9178, Val Loss: 0.0816\nEpoch 5, Train Loss: 4.7080, Val Loss: 0.0153\nEpoch 6, Train Loss: 0.5674, Val Loss: 0.0048\nEpoch 7, Train Loss: 0.2469, Val Loss: 0.0040\nEpoch 8, Train Loss: 0.1972, Val Loss: 0.0030\nEpoch 9, Train Loss: 0.1826, Val Loss: 0.0030\nEpoch 10, Train Loss: 0.1564, Val Loss: 0.0025\nEpoch 11, Train Loss: 0.1370, Val Loss: 0.0024\nEpoch 12, Train Loss: 0.1257, Val Loss: 0.0025\nEpoch 13, Train Loss: 0.1176, Val Loss: 0.0025\nEpoch 14, Train Loss: 0.1078, Val Loss: 0.0021\nEpoch 15, Train Loss: 0.1010, Val Loss: 0.0022\nEpoch 16, Train Loss: 0.1061, Val Loss: 0.0021\nEpoch 17, Train Loss: 0.0832, Val Loss: 0.0021\nEpoch 18, Train Loss: 0.0778, Val Loss: 0.0017\nEpoch 19, Train Loss: 0.0666, Val Loss: 0.0023\nEpoch 20, Train Loss: 0.0641, Val Loss: 0.0015\nEpoch 21, Train Loss: 0.0583, Val Loss: 0.0014\nEpoch 22, Train Loss: 0.0466, Val Loss: 0.0013\nEpoch 23, Train Loss: 0.0436, Val Loss: 0.0012\nEpoch 24, Train Loss: 0.0473, Val Loss: 0.0016\nEpoch 25, Train Loss: 0.0435, Val Loss: 0.0014\nEpoch 26, Train Loss: 0.0363, Val Loss: 0.0012\nEpoch 27, Train Loss: 0.0343, Val Loss: 0.0012\nEpoch 28, Train Loss: 0.0306, Val Loss: 0.0012\nEpoch 29, Train Loss: 0.0305, Val Loss: 0.0012\nEpoch 30, Train Loss: 0.0290, Val Loss: 0.0010\nEpoch 31, Train Loss: 0.0262, Val Loss: 0.0011\nEpoch 32, Train Loss: 0.0248, Val Loss: 0.0012\nEpoch 33, Train Loss: 0.0245, Val Loss: 0.0012\nEpoch 34, Train Loss: 0.0224, Val Loss: 0.0015\nEpoch 35, Train Loss: 0.0225, Val Loss: 0.0011\nEpoch 36, Train Loss: 0.0202, Val Loss: 0.0011\nEpoch 37, Train Loss: 0.0176, Val Loss: 0.0010\nEpoch 38, Train Loss: 0.0170, Val Loss: 0.0010\nEpoch 39, Train Loss: 0.0163, Val Loss: 0.0010\nEpoch 40, Train Loss: 0.0174, Val Loss: 0.0012\nEpoch 41, Train Loss: 0.0174, Val Loss: 0.0011\nEpoch 42, Train Loss: 0.0126, Val Loss: 0.0010\nEpoch 43, Train Loss: 0.0129, Val Loss: 0.0011\nEpoch 44, Train Loss: 0.0150, Val Loss: 0.0010\nEpoch 45, Train Loss: 0.0104, Val Loss: 0.0011\nEpoch 46, Train Loss: 0.0124, Val Loss: 0.0010\nEpoch 47, Train Loss: 0.0127, Val Loss: 0.0010\nEpoch 48, Train Loss: 0.0091, Val Loss: 0.0010\nEpoch 49, Train Loss: 0.0087, Val Loss: 0.0010\nEpoch 50, Train Loss: 0.0073, Val Loss: 0.0009\nEpoch 51, Train Loss: 0.0084, Val Loss: 0.0010\nEpoch 52, Train Loss: 0.0067, Val Loss: 0.0010\nEpoch 53, Train Loss: 0.0092, Val Loss: 0.0010\nEpoch 54, Train Loss: 0.0076, Val Loss: 0.0011\nEpoch 55, Train Loss: 0.0100, Val Loss: 0.0010\nEpoch 56, Train Loss: 0.0078, Val Loss: 0.0010\nEpoch 57, Train Loss: 0.0050, Val Loss: 0.0010\nEpoch 58, Train Loss: 0.0081, Val Loss: 0.0010\nEpoch 59, Train Loss: 0.0085, Val Loss: 0.0010\nEpoch 60, Train Loss: 0.0068, Val Loss: 0.0010\nEpoch 61, Train Loss: 0.0053, Val Loss: 0.0010\nEpoch 62, Train Loss: 0.0066, Val Loss: 0.0009\nEpoch 63, Train Loss: 0.0065, Val Loss: 0.0010\nEpoch 64, Train Loss: 0.0068, Val Loss: 0.0009\nEpoch 65, Train Loss: 0.0125, Val Loss: 0.0010\nEpoch 66, Train Loss: 0.0096, Val Loss: 0.0010\nEpoch 67, Train Loss: 0.0092, Val Loss: 0.0010\nEpoch 68, Train Loss: 0.0100, Val Loss: 0.0010\nEpoch 69, Train Loss: 0.0088, Val Loss: 0.0009\nEpoch 70, Train Loss: 0.0063, Val Loss: 0.0009\nEpoch 71, Train Loss: 0.0072, Val Loss: 0.0010\nEpoch 72, Train Loss: 0.0153, Val Loss: 0.0010\nEpoch 73, Train Loss: 0.0072, Val Loss: 0.0009\nEpoch 74, Train Loss: 0.0058, Val Loss: 0.0009\nEpoch 75, Train Loss: 0.0084, Val Loss: 0.0009\nEpoch 76, Train Loss: 0.0099, Val Loss: 0.0009\nEpoch 77, Train Loss: 0.0095, Val Loss: 0.0009\nEpoch 78, Train Loss: 0.0092, Val Loss: 0.0010\nEpoch 79, Train Loss: 0.0074, Val Loss: 0.0010\nEpoch 80, Train Loss: 0.0074, Val Loss: 0.0009\nEpoch 81, Train Loss: 0.0046, Val Loss: 0.0008\nEpoch 82, Train Loss: 0.0050, Val Loss: 0.0009\nEpoch 83, Train Loss: 0.0044, Val Loss: 0.0009\nEpoch 84, Train Loss: 0.0036, Val Loss: 0.0009\nEpoch 85, Train Loss: 0.0041, Val Loss: 0.0008\nEpoch 86, Train Loss: 0.0036, Val Loss: 0.0008\nEpoch 87, Train Loss: 0.0072, Val Loss: 0.0009\nEpoch 88, Train Loss: 0.0054, Val Loss: 0.0009\nEpoch 89, Train Loss: 0.0057, Val Loss: 0.0009\nEpoch 90, Train Loss: 0.0083, Val Loss: 0.0009\nEpoch 91, Train Loss: 0.0082, Val Loss: 0.0010\nEpoch 92, Train Loss: 0.0114, Val Loss: 0.0009\nEpoch 93, Train Loss: 0.0101, Val Loss: 0.0009\nEpoch 94, Train Loss: 0.0070, Val Loss: 0.0008\nEpoch 95, Train Loss: 0.0068, Val Loss: 0.0009\nEpoch 96, Train Loss: 0.0067, Val Loss: 0.0008\nEpoch 97, Train Loss: 0.0061, Val Loss: 0.0009\nEpoch 98, Train Loss: 0.0053, Val Loss: 0.0008\nEpoch 99, Train Loss: 0.0050, Val Loss: 0.0009\nEpoch 100, Train Loss: 0.0052, Val Loss: 0.0009\nEpoch 101, Train Loss: 0.0046, Val Loss: 0.0008\nEpoch 102, Train Loss: 0.0058, Val Loss: 0.0009\nEpoch 103, Train Loss: 0.0083, Val Loss: 0.0008\nEpoch 104, Train Loss: 0.0076, Val Loss: 0.0008\nEpoch 105, Train Loss: 0.0049, Val Loss: 0.0008\nEpoch 106, Train Loss: 0.0038, Val Loss: 0.0008\nEpoch 107, Train Loss: 0.0039, Val Loss: 0.0008\nEpoch 108, Train Loss: 0.0047, Val Loss: 0.0008\nEpoch 109, Train Loss: 0.0041, Val Loss: 0.0008\nEpoch 110, Train Loss: 0.0054, Val Loss: 0.0007\nEpoch 111, Train Loss: 0.0058, Val Loss: 0.0008\nEpoch 112, Train Loss: 0.0069, Val Loss: 0.0009\nEpoch 113, Train Loss: 0.0071, Val Loss: 0.0008\nEpoch 114, Train Loss: 0.0138, Val Loss: 0.0009\nEpoch 115, Train Loss: 0.0087, Val Loss: 0.0008\nEpoch 116, Train Loss: 0.0073, Val Loss: 0.0008\nEpoch 117, Train Loss: 0.0067, Val Loss: 0.0007\nEpoch 118, Train Loss: 0.0050, Val Loss: 0.0008\nEpoch 119, Train Loss: 0.0071, Val Loss: 0.0007\nEpoch 120, Train Loss: 0.0055, Val Loss: 0.0008\nEpoch 121, Train Loss: 0.0041, Val Loss: 0.0008\nEpoch 122, Train Loss: 0.0041, Val Loss: 0.0007\nEpoch 123, Train Loss: 0.0031, Val Loss: 0.0007\nEpoch 124, Train Loss: 0.0036, Val Loss: 0.0007\nEpoch 125, Train Loss: 0.0056, Val Loss: 0.0008\nEpoch 126, Train Loss: 0.0055, Val Loss: 0.0007\nEpoch 127, Train Loss: 0.0074, Val Loss: 0.0007\nEpoch 128, Train Loss: 0.0139, Val Loss: 0.0009\nEpoch 129, Train Loss: 0.0125, Val Loss: 0.0007\nEpoch 130, Train Loss: 0.0115, Val Loss: 0.0007\nEpoch 131, Train Loss: 0.0073, Val Loss: 0.0007\nEpoch 132, Train Loss: 0.0048, Val Loss: 0.0007\nEpoch 133, Train Loss: 0.0032, Val Loss: 0.0007\nEpoch 134, Train Loss: 0.0027, Val Loss: 0.0007\nEpoch 135, Train Loss: 0.0027, Val Loss: 0.0007\nEpoch 136, Train Loss: 0.0026, Val Loss: 0.0007\nEpoch 137, Train Loss: 0.0021, Val Loss: 0.0007\nEpoch 138, Train Loss: 0.0026, Val Loss: 0.0007\nEpoch 139, Train Loss: 0.0025, Val Loss: 0.0006\nEpoch 140, Train Loss: 0.0023, Val Loss: 0.0007\nEpoch 141, Train Loss: 0.0026, Val Loss: 0.0007\nEpoch 142, Train Loss: 0.0030, Val Loss: 0.0007\nEpoch 143, Train Loss: 0.0037, Val Loss: 0.0007\nEpoch 144, Train Loss: 0.0034, Val Loss: 0.0007\nEpoch 145, Train Loss: 0.0034, Val Loss: 0.0007\nEpoch 146, Train Loss: 0.0039, Val Loss: 0.0006\nEpoch 147, Train Loss: 0.0035, Val Loss: 0.0007\nEpoch 148, Train Loss: 0.0043, Val Loss: 0.0007\nEpoch 149, Train Loss: 0.0075, Val Loss: 0.0008\nEpoch 150, Train Loss: 0.0097, Val Loss: 0.0008\nEpoch 151, Train Loss: 0.0075, Val Loss: 0.0007\nEpoch 152, Train Loss: 0.0068, Val Loss: 0.0007\nEpoch 153, Train Loss: 0.0068, Val Loss: 0.0007\nEpoch 154, Train Loss: 0.0053, Val Loss: 0.0006\nEpoch 155, Train Loss: 0.0040, Val Loss: 0.0008\nEpoch 156, Train Loss: 0.0042, Val Loss: 0.0007\nEpoch 157, Train Loss: 0.0033, Val Loss: 0.0007\nEpoch 158, Train Loss: 0.0033, Val Loss: 0.0007\nEpoch 159, Train Loss: 0.0038, Val Loss: 0.0006\nEpoch 160, Train Loss: 0.0036, Val Loss: 0.0007\nEpoch 161, Train Loss: 0.0041, Val Loss: 0.0007\nEpoch 162, Train Loss: 0.0036, Val Loss: 0.0007\nEpoch 163, Train Loss: 0.0030, Val Loss: 0.0006\nEpoch 164, Train Loss: 0.0030, Val Loss: 0.0007\nEpoch 165, Train Loss: 0.0026, Val Loss: 0.0007\nEpoch 166, Train Loss: 0.0033, Val Loss: 0.0007\nEpoch 167, Train Loss: 0.0029, Val Loss: 0.0007\nEpoch 168, Train Loss: 0.0029, Val Loss: 0.0007\nEpoch 169, Train Loss: 0.0035, Val Loss: 0.0007\nEpoch 170, Train Loss: 0.0036, Val Loss: 0.0006\nEpoch 171, Train Loss: 0.0034, Val Loss: 0.0006\nEpoch 172, Train Loss: 0.0049, Val Loss: 0.0006\nEpoch 173, Train Loss: 0.0074, Val Loss: 0.0006\nEpoch 174, Train Loss: 0.0101, Val Loss: 0.0009\nEpoch 175, Train Loss: 0.0096, Val Loss: 0.0008\nEpoch 176, Train Loss: 0.0059, Val Loss: 0.0007\nEpoch 177, Train Loss: 0.0037, Val Loss: 0.0006\nEpoch 178, Train Loss: 0.0026, Val Loss: 0.0006\nEpoch 179, Train Loss: 0.0020, Val Loss: 0.0006\nEpoch 180, Train Loss: 0.0020, Val Loss: 0.0007\nEpoch 181, Train Loss: 0.0019, Val Loss: 0.0006\nEpoch 182, Train Loss: 0.0018, Val Loss: 0.0006\nEpoch 183, Train Loss: 0.0017, Val Loss: 0.0006\nEpoch 184, Train Loss: 0.0014, Val Loss: 0.0006\nEpoch 185, Train Loss: 0.0020, Val Loss: 0.0006\nEpoch 186, Train Loss: 0.0024, Val Loss: 0.0006\nEpoch 187, Train Loss: 0.0024, Val Loss: 0.0006\nEpoch 188, Train Loss: 0.0028, Val Loss: 0.0006\nEpoch 189, Train Loss: 0.0025, Val Loss: 0.0006\nEpoch 190, Train Loss: 0.0054, Val Loss: 0.0006\nEpoch 191, Train Loss: 0.0075, Val Loss: 0.0007\nEpoch 192, Train Loss: 0.0070, Val Loss: 0.0006\nEpoch 193, Train Loss: 0.0046, Val Loss: 0.0006\nEpoch 194, Train Loss: 0.0026, Val Loss: 0.0006\nEpoch 195, Train Loss: 0.0024, Val Loss: 0.0006\nEpoch 196, Train Loss: 0.0030, Val Loss: 0.0006\nEpoch 197, Train Loss: 0.0040, Val Loss: 0.0006\nEpoch 198, Train Loss: 0.0036, Val Loss: 0.0006\nEpoch 199, Train Loss: 0.0028, Val Loss: 0.0006\nEpoch 200, Train Loss: 0.0033, Val Loss: 0.0006\nEpoch 201, Train Loss: 0.0031, Val Loss: 0.0006\nEpoch 202, Train Loss: 0.0035, Val Loss: 0.0006\nEpoch 203, Train Loss: 0.0033, Val Loss: 0.0006\nEpoch 204, Train Loss: 0.0025, Val Loss: 0.0006\nEpoch 205, Train Loss: 0.0028, Val Loss: 0.0006\nEpoch 206, Train Loss: 0.0018, Val Loss: 0.0006\nEpoch 207, Train Loss: 0.0025, Val Loss: 0.0006\nEpoch 208, Train Loss: 0.0029, Val Loss: 0.0006\nEpoch 209, Train Loss: 0.0042, Val Loss: 0.0006\nEpoch 210, Train Loss: 0.0035, Val Loss: 0.0006\nEpoch 211, Train Loss: 0.0031, Val Loss: 0.0006\nEpoch 212, Train Loss: 0.0027, Val Loss: 0.0006\nEpoch 213, Train Loss: 0.0030, Val Loss: 0.0006\nEpoch 214, Train Loss: 0.0029, Val Loss: 0.0006\nEpoch 215, Train Loss: 0.0034, Val Loss: 0.0006\nEpoch 216, Train Loss: 0.0042, Val Loss: 0.0006\nEpoch 217, Train Loss: 0.0052, Val Loss: 0.0006\nEpoch 218, Train Loss: 0.0039, Val Loss: 0.0006\nEpoch 219, Train Loss: 0.0040, Val Loss: 0.0006\nEpoch 220, Train Loss: 0.0032, Val Loss: 0.0006\nEpoch 221, Train Loss: 0.0022, Val Loss: 0.0006\nEpoch 222, Train Loss: 0.0021, Val Loss: 0.0006\nEpoch 223, Train Loss: 0.0020, Val Loss: 0.0006\nEpoch 224, Train Loss: 0.0014, Val Loss: 0.0006\nEpoch 225, Train Loss: 0.0015, Val Loss: 0.0006\nEpoch 226, Train Loss: 0.0015, Val Loss: 0.0005\nEpoch 227, Train Loss: 0.0012, Val Loss: 0.0006\nEpoch 228, Train Loss: 0.0018, Val Loss: 0.0006\nEpoch 229, Train Loss: 0.0030, Val Loss: 0.0006\nEpoch 230, Train Loss: 0.0037, Val Loss: 0.0006\nEpoch 231, Train Loss: 0.0034, Val Loss: 0.0006\nEpoch 232, Train Loss: 0.0042, Val Loss: 0.0006\nEpoch 233, Train Loss: 0.0048, Val Loss: 0.0006\nEpoch 234, Train Loss: 0.0043, Val Loss: 0.0006\nEpoch 235, Train Loss: 0.0032, Val Loss: 0.0006\nEpoch 236, Train Loss: 0.0029, Val Loss: 0.0006\nEpoch 237, Train Loss: 0.0016, Val Loss: 0.0006\nEpoch 238, Train Loss: 0.0015, Val Loss: 0.0006\nEpoch 239, Train Loss: 0.0015, Val Loss: 0.0006\nEpoch 240, Train Loss: 0.0018, Val Loss: 0.0006\nEpoch 241, Train Loss: 0.0023, Val Loss: 0.0006\nEpoch 242, Train Loss: 0.0019, Val Loss: 0.0006\nEpoch 243, Train Loss: 0.0024, Val Loss: 0.0006\nEpoch 244, Train Loss: 0.0036, Val Loss: 0.0006\nEpoch 245, Train Loss: 0.0037, Val Loss: 0.0006\nEpoch 246, Train Loss: 0.0030, Val Loss: 0.0006\nEarly stopping at epoch 246\nMean Absolute Percentage Accuracy (MAPA): 89.56%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nmodel.eval()\n\n# Fine-tuning the model with the new dataset\nfine_tune_epochs = 100\nfor epoch in range(fine_tune_epochs):\n    model.train()\n    total_loss = 0\n    for batch_x, batch_y in train_new_loader:  # Use train_new_loader\n        batch_x = batch_x.long()  # Ensure the batch is in the correct type (long tensor for indices)\n        optimizer.zero_grad()\n        output = model(batch_x).squeeze()  # Forward pass\n        loss = criterion(output, batch_y)  # Compute loss\n        loss.backward()  # Backpropagate\n        optimizer.step()  # Update weights\n        total_loss += loss.item()  # Accumulate the loss\n    print(f\"Fine-tune Epoch {epoch+1}/{fine_tune_epochs}, Loss: {total_loss:.4f}\")\n\n# Evaluation on new test set\nmodel.eval()\nactuals = []\npredictions = []\n\nwith torch.no_grad():\n    for batch_x, batch_y in test_new_loader:  # Evaluate on test_new_loader\n        batch_x = batch_x.long()  # Ensure the batch is in the correct type (long tensor for indices)\n        output = model(batch_x).squeeze()\n        actuals.extend(batch_y.cpu().numpy())\n        predictions.extend(output.cpu().numpy())\n\nactuals = np.array(actuals)\npredictions = np.array(predictions)\n\n# Avoid division by zero\nnonzero_mask = actuals != 0\nmape = np.mean(np.abs((actuals[nonzero_mask] - predictions[nonzero_mask]) / actuals[nonzero_mask])) * 100\nmapa = 100 - mape\n\nprint(f\"Mean Absolute Percentage Accuracy (MAPA): {mapa:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:32:07.615260Z","iopub.execute_input":"2025-04-23T11:32:07.616173Z","iopub.status.idle":"2025-04-23T11:32:30.354898Z","shell.execute_reply.started":"2025-04-23T11:32:07.616146Z","shell.execute_reply":"2025-04-23T11:32:30.354137Z"}},"outputs":[{"name":"stdout","text":"Fine-tune Epoch 1/100, Loss: 50.1976\nFine-tune Epoch 2/100, Loss: 11.0782\nFine-tune Epoch 3/100, Loss: 0.8229\nFine-tune Epoch 4/100, Loss: 0.3238\nFine-tune Epoch 5/100, Loss: 0.2361\nFine-tune Epoch 6/100, Loss: 0.1464\nFine-tune Epoch 7/100, Loss: 0.1179\nFine-tune Epoch 8/100, Loss: 0.1073\nFine-tune Epoch 9/100, Loss: 0.0916\nFine-tune Epoch 10/100, Loss: 0.0819\nFine-tune Epoch 11/100, Loss: 0.0791\nFine-tune Epoch 12/100, Loss: 0.0731\nFine-tune Epoch 13/100, Loss: 0.0691\nFine-tune Epoch 14/100, Loss: 0.0667\nFine-tune Epoch 15/100, Loss: 0.0630\nFine-tune Epoch 16/100, Loss: 0.0607\nFine-tune Epoch 17/100, Loss: 0.0562\nFine-tune Epoch 18/100, Loss: 0.0556\nFine-tune Epoch 19/100, Loss: 0.0507\nFine-tune Epoch 20/100, Loss: 0.0525\nFine-tune Epoch 21/100, Loss: 0.0467\nFine-tune Epoch 22/100, Loss: 0.0437\nFine-tune Epoch 23/100, Loss: 0.0441\nFine-tune Epoch 24/100, Loss: 0.0401\nFine-tune Epoch 25/100, Loss: 0.0424\nFine-tune Epoch 26/100, Loss: 0.0385\nFine-tune Epoch 27/100, Loss: 0.0370\nFine-tune Epoch 28/100, Loss: 0.0349\nFine-tune Epoch 29/100, Loss: 0.0347\nFine-tune Epoch 30/100, Loss: 0.0331\nFine-tune Epoch 31/100, Loss: 0.0315\nFine-tune Epoch 32/100, Loss: 0.0323\nFine-tune Epoch 33/100, Loss: 0.0288\nFine-tune Epoch 34/100, Loss: 0.0277\nFine-tune Epoch 35/100, Loss: 0.0266\nFine-tune Epoch 36/100, Loss: 0.0265\nFine-tune Epoch 37/100, Loss: 0.0243\nFine-tune Epoch 38/100, Loss: 0.0282\nFine-tune Epoch 39/100, Loss: 0.0271\nFine-tune Epoch 40/100, Loss: 0.0232\nFine-tune Epoch 41/100, Loss: 0.0203\nFine-tune Epoch 42/100, Loss: 0.0206\nFine-tune Epoch 43/100, Loss: 0.0198\nFine-tune Epoch 44/100, Loss: 0.0187\nFine-tune Epoch 45/100, Loss: 0.0192\nFine-tune Epoch 46/100, Loss: 0.0189\nFine-tune Epoch 47/100, Loss: 0.0164\nFine-tune Epoch 48/100, Loss: 0.0149\nFine-tune Epoch 49/100, Loss: 0.0165\nFine-tune Epoch 50/100, Loss: 0.0190\nFine-tune Epoch 51/100, Loss: 0.0138\nFine-tune Epoch 52/100, Loss: 0.0134\nFine-tune Epoch 53/100, Loss: 0.0120\nFine-tune Epoch 54/100, Loss: 0.0124\nFine-tune Epoch 55/100, Loss: 0.0108\nFine-tune Epoch 56/100, Loss: 0.0107\nFine-tune Epoch 57/100, Loss: 0.0101\nFine-tune Epoch 58/100, Loss: 0.0101\nFine-tune Epoch 59/100, Loss: 0.0091\nFine-tune Epoch 60/100, Loss: 0.0094\nFine-tune Epoch 61/100, Loss: 0.0090\nFine-tune Epoch 62/100, Loss: 0.0082\nFine-tune Epoch 63/100, Loss: 0.0089\nFine-tune Epoch 64/100, Loss: 0.0085\nFine-tune Epoch 65/100, Loss: 0.0082\nFine-tune Epoch 66/100, Loss: 0.0077\nFine-tune Epoch 67/100, Loss: 0.0073\nFine-tune Epoch 68/100, Loss: 0.0068\nFine-tune Epoch 69/100, Loss: 0.0065\nFine-tune Epoch 70/100, Loss: 0.0055\nFine-tune Epoch 71/100, Loss: 0.0051\nFine-tune Epoch 72/100, Loss: 0.0058\nFine-tune Epoch 73/100, Loss: 0.0049\nFine-tune Epoch 74/100, Loss: 0.0049\nFine-tune Epoch 75/100, Loss: 0.0046\nFine-tune Epoch 76/100, Loss: 0.0041\nFine-tune Epoch 77/100, Loss: 0.0043\nFine-tune Epoch 78/100, Loss: 0.0054\nFine-tune Epoch 79/100, Loss: 0.0043\nFine-tune Epoch 80/100, Loss: 0.0040\nFine-tune Epoch 81/100, Loss: 0.0045\nFine-tune Epoch 82/100, Loss: 0.0037\nFine-tune Epoch 83/100, Loss: 0.0035\nFine-tune Epoch 84/100, Loss: 0.0030\nFine-tune Epoch 85/100, Loss: 0.0029\nFine-tune Epoch 86/100, Loss: 0.0026\nFine-tune Epoch 87/100, Loss: 0.0026\nFine-tune Epoch 88/100, Loss: 0.0025\nFine-tune Epoch 89/100, Loss: 0.0024\nFine-tune Epoch 90/100, Loss: 0.0024\nFine-tune Epoch 91/100, Loss: 0.0022\nFine-tune Epoch 92/100, Loss: 0.0022\nFine-tune Epoch 93/100, Loss: 0.0021\nFine-tune Epoch 94/100, Loss: 0.0021\nFine-tune Epoch 95/100, Loss: 0.0017\nFine-tune Epoch 96/100, Loss: 0.0016\nFine-tune Epoch 97/100, Loss: 0.0016\nFine-tune Epoch 98/100, Loss: 0.0015\nFine-tune Epoch 99/100, Loss: 0.0015\nFine-tune Epoch 100/100, Loss: 0.0015\nMean Absolute Percentage Accuracy (MAPA): 96.82%\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}